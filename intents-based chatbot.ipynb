{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"intents-based chatbot.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"14ybrq9SsZ5Oef-axVQfGIYhUd6p4kI1H","authorship_tag":"ABX9TyNBI/XWDQ9KrMA7jRn5N5Gc"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"y1WKZ2oo7ebs"},"source":["# Import libraries"]},{"cell_type":"code","metadata":{"id":"Peuf2hHvTffa","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1643236545041,"user_tz":0,"elapsed":11908,"user":{"displayName":"Ruei-Huan Rao","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13183932021173535053"}},"outputId":"0cce96f5-efa8-4ba4-e971-bb30f343a1f4"},"source":["import numpy as np\n","import random\n","import json\n","\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import Dataset, DataLoader\n","\n","import nltk\n","nltk.download('punkt')\n","from nltk.stem.porter import PorterStemmer\n","stemmer = PorterStemmer()\n","\n","# check if GPU is available\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print(device)"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","cuda\n"]}]},{"cell_type":"markdown","metadata":{"id":"N4StMv7o6QdD"},"source":["# Build the functions"]},{"cell_type":"code","metadata":{"id":"mJNfFnzw6ZXt","executionInfo":{"status":"ok","timestamp":1643236545042,"user_tz":0,"elapsed":6,"user":{"displayName":"Ruei-Huan Rao","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13183932021173535053"}}},"source":["def tokenise(sentence):\n","    \"\"\"\n","    split sentence into an array of tokens\n","    \"\"\"\n","    return nltk.word_tokenize(sentence)\n","\n","def lower_stem(word):\n","    \"\"\"\n","    stemming = find the root form of the word\n","    words = [\"organize\", \"organizes\", \"organizing\"]\n","          -> [\"organ\", \"organ\", \"organ\"]\n","    \"\"\"\n","    return stemmer.stem(word.lower())\n","\n","def bag_of_words(tokenised_sentence, words):\n","    \"\"\"\n","    return bag of words array: 1 indicates each known word that exists in the sentence, 0 otherwise\n","    tokened_sentence = ['may', 'I', 'have', 'a', 'look']\n","    words = [\"hi\", \"have\", \"how\", \"may\", \"you\", \"bye\", \"a\", \"wonder\"]\n","    bag   = [ 0,      1,     0,     1,     0,     0,    1,      0   ]   \n","    \"\"\"\n","    # stem each word\n","    sentence_words = [lower_stem(word) for word in tokenised_sentence]\n","    # initialize bag with 0 for each word\n","    bag = np.zeros(len(words), dtype=np.float32)\n","    for idx, w in enumerate(words):\n","        if w in sentence_words: \n","            bag[idx] = 1\n","    return bag"],"execution_count":2,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2wEkcoTu6f9D"},"source":["# Build the model"]},{"cell_type":"code","metadata":{"id":"QSHgtqEm6b_S","executionInfo":{"status":"ok","timestamp":1643236545042,"user_tz":0,"elapsed":5,"user":{"displayName":"Ruei-Huan Rao","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13183932021173535053"}}},"source":["'''\n","build neural network:\n","  - three linear layers\n","  - activation layer: ReLU\n","  - dropout ratio: 0.15\n","'''\n","class NN(nn.Module):\n","    def __init__(self, input_size, hidden_size, output_size):\n","        super().__init__()\n","        self.linear_layer_1 = nn.Linear(input_size, hidden_size) \n","        self.linear_layer_2 = nn.Linear(hidden_size, hidden_size) \n","        self.linear_layer_3 = nn.Linear(hidden_size, output_size)\n","        self.relu = nn.ReLU()\n","        self.dropout = nn.Dropout(0.15)\n","    \n","    def forward(self, x):\n","        out = self.linear_layer_1(x)\n","        out = self.dropout(out)\n","        out = self.relu(out)\n","        out = self.linear_layer_2(out)\n","        out = self.dropout(out)\n","        out = self.relu(out)\n","        out = self.linear_layer_3(out)\n","        return out"],"execution_count":3,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_JCsjWpd7wQZ"},"source":["# Load and build the dataset"]},{"cell_type":"code","metadata":{"id":"flaXqv7RL54v","executionInfo":{"status":"ok","timestamp":1643236546075,"user_tz":0,"elapsed":1037,"user":{"displayName":"Ruei-Huan Rao","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13183932021173535053"}}},"source":["intents_path = \"/content/drive/MyDrive/UCD_Programmes/Summer/ACM40960_Projects_in_Maths_Modelling/My_Project/dataset/intents.json\"\n","with open(intents_path, 'r') as f:\n","    intents = json.load(f)"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"8K_rpPfROJlx","executionInfo":{"status":"ok","timestamp":1643236546076,"user_tz":0,"elapsed":10,"user":{"displayName":"Ruei-Huan Rao","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13183932021173535053"}}},"source":["all_words = []\n","tags = []\n","xy = []\n","# loop through each sentence in our intents patterns\n","for intent in intents['intents']:\n","  tag = intent['tag']\n","  # add to tag list\n","  tags.append(tag)\n","  for pattern in intent['patterns']:\n","    # tokenize each word in the sentence\n","    w = tokenise(pattern)\n","    # add to our words list\n","    all_words.extend(w)\n","    # add to xy pair\n","    xy.append((w, tag))"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"NA5bc_zrQD0h","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1643236546077,"user_tz":0,"elapsed":10,"user":{"displayName":"Ruei-Huan Rao","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13183932021173535053"}},"outputId":"9a2af82c-a896-4902-eae8-368160db25c9"},"source":["# lower, stem and eliminate punctuation words \n","all_words = [lower_stem(w) for w in all_words if w.isalnum()]\n","\n","# remove duplicates and sort\n","all_words = sorted(set(all_words))\n","tags = sorted(set(tags))\n","\n","print(len(xy), \"patterns\")\n","print(len(tags), \"tags:\", tags)\n","print(len(all_words), \"unique stemmed words:\", all_words)"],"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["128 patterns\n","47 tags: ['HR_related_problem', 'Location', 'Weather', 'about', 'appointment status', 'cabin', 'check_leave', 'commission', 'competitors_in_market', 'configuration', 'connect_people', 'cost_lowering', 'customer_satisfaction', 'delivery', 'domain', 'email_id', 'factors_impacting_sale', 'forgot_password', 'funny', 'gadgets', 'goodbye', 'greeting', 'highest_grossing', 'hours', 'invalid', 'items', 'key_customers', 'leave', 'maintainence', 'manufacturing_problems', 'missing_id', 'name', 'noanswer', 'options', 'order_components', 'order_tracking', 'payments', 'predict_delay', 'predict_performance', 'project_handling_queries', 'search_department', 'search_person_by_id', 'solve_problems', 'supplier_info', 'thanks', 'turnover', 'version_update']\n","235 unique stemmed words: ['23a12', '23a31', '32712', '345a23', '431b67', '561a24', '562b78', 'a', 'abc', 'abx', 'accept', 'accid', 'ai', 'am', 'an', 'analysi', 'and', 'anyon', 'appoint', 'appoit', 'are', 'at', 'base', 'be', 'been', 'benefit', 'bhatt', 'bore', 'bye', 'cabin', 'cafeteria', 'call', 'can', 'canteen', 'card', 'cash', 'challeng', 'chang', 'chat', 'clariti', 'clear', 'commiss', 'compani', 'compens', 'complaint', 'compon', 'compris', 'comput', 'configur', 'conflict', 'cost', 'could', 'credit', 'custom', 'date', 'day', 'delay', 'deliveri', 'demand', 'depart', 'design', 'desktop', 'develop', 'do', 'doe', 'domain', 'each', 'employe', 'event', 'factor', 'feedback', 'find', 'fix', 'for', 'forget', 'forgot', 'from', 'funni', 'gadget', 'get', 'good', 'goodby', 'gross', 'guid', 'ha', 'handl', 'happi', 'have', 'head', 'hello', 'help', 'hey', 'hi', 'highest', 'hola', 'hour', 'how', 'i', 'id', 'impact', 'improv', 'in', 'inform', 'insuffici', 'is', 'issu', 'it', 'item', 'job', 'joke', 'key', 'kind', 'know', 'knowledg', 'kumar', 'lack', 'laptop', 'last', 'later', 'leav', 'legal', 'like', 'list', 'locat', 'login', 'long', 'lot', 'love', 'lower', 'made', 'maintain', 'manag', 'manish', 'manoj', 'manufactur', 'market', 'marri', 'mastercard', 'me', 'meet', 'member', 'michel', 'miscommun', 'my', 'name', 'nandi', 'nearbi', 'need', 'not', 'occur', 'of', 'offic', 'on', 'onli', 'open', 'order', 'our', 'password', 'pay', 'paypal', 'plan', 'present', 'problem', 'product', 'profit', 'project', 'provid', 'queri', 'rais', 'rakesh', 'rate', 'record', 'relat', 'resolv', 'respons', 'risk', 'roy', 'sale', 'see', 'sell', 'set', 'shantanu', 'share', 'sharma', 'ship', 'should', 'siddhart', 'skill', 'softwar', 'someon', 'someth', 'specif', 'step', 'stock', 'sujata', 'supplier', 'take', 'target', 'team', 'tell', 'thank', 'that', 'the', 'there', 'thi', 'threat', 'time', 'to', 'today', 'track', 'train', 'turnov', 'updat', 'urgent', 'user', 'variou', 'version', 'vp', 'wa', 'want', 'we', 'weather', 'what', 'when', 'where', 'whi', 'which', 'who', 'wifi', 'with', 'work', 'workforc', 'year', 'you', 'your']\n"]}]},{"cell_type":"code","metadata":{"id":"0QJsq9J2VwoM","executionInfo":{"status":"ok","timestamp":1643236546077,"user_tz":0,"elapsed":5,"user":{"displayName":"Ruei-Huan Rao","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13183932021173535053"}}},"source":["# create training data\n","x_train = []\n","y_train = []\n","for (tokenised_sentence, tag) in xy:\n","  # x: bag of words for each pattern_sentence\n","  bag = bag_of_words(tokenised_sentence, all_words)\n","  x_train.append(bag)\n","  # y: PyTorch CrossEntropyLoss needs only class labels, not one-hot\n","  label = tags.index(tag)\n","  y_train.append(label)\n","\n","x_train = np.array(x_train)\n","y_train = np.array(y_train)"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"G-iNXo1BY0gC","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1643236546836,"user_tz":0,"elapsed":763,"user":{"displayName":"Ruei-Huan Rao","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13183932021173535053"}},"outputId":"6100b1c7-fafb-4939-d774-e2aefb18b7bc"},"source":["print(x_train.shape)\n","print(y_train.shape)"],"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["(128, 235)\n","(128,)\n"]}]},{"cell_type":"code","metadata":{"id":"SycrEiaHORqI","executionInfo":{"status":"ok","timestamp":1643236549350,"user_tz":0,"elapsed":3,"user":{"displayName":"Ruei-Huan Rao","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13183932021173535053"}}},"source":["class ChatDataset(Dataset):\n","  def __init__(self):\n","    self.n_samples = len(x_train)\n","    self.x_data = x_train\n","    self.y_data = y_train\n","\n","  # support indexing such that dataset[i] can be used to get i-th sample\n","  def __getitem__(self, index):\n","    return self.x_data[index], self.y_data[index]\n","\n","  # we can call len(dataset) to return the size\n","  def __len__(self):\n","    return self.n_samples"],"execution_count":9,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3chep7Wn9ku6"},"source":["# Model training"]},{"cell_type":"code","metadata":{"id":"jG2cZ-BPTV5T","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1643236594744,"user_tz":0,"elapsed":13793,"user":{"displayName":"Ruei-Huan Rao","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13183932021173535053"}},"outputId":"ca8b84a5-a8ab-47fc-cf45-9d98a87984fe"},"source":["# set hyper-parameters \n","epochs = 200\n","batch_size = 5\n","learning_rate = 0.001\n","input_size = x_train.shape[1]\n","hidden_size = 64\n","output_size = len(tags)\n","print(input_size, output_size)\n","\n","# build the training dataset\n","dataset = ChatDataset()\n","train_loader = DataLoader(dataset=dataset,\n","                          batch_size=batch_size,\n","                          shuffle=True,\n","                          num_workers=0)\n","\n","# initialise the model\n","model = NN(input_size, hidden_size, output_size).to(device)\n","\n","# define loss function\n","criterion = nn.CrossEntropyLoss()\n","\n","# define optimisation function\n","optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n","\n","# start training the model\n","for epoch in range(epoches):\n","  for (words, labels) in train_loader:\n","\n","    # initialise the gradient cache\n","    optimizer.zero_grad()\n","\n","    # run in the device (either GPU or CPU)\n","    words = words.to(device)\n","    labels = labels.to(dtype=torch.long).to(device)\n","    \n","    # Forward pass\n","    outputs = model(words)\n","\n","    # calculate the loss\n","    loss = criterion(outputs, labels)\n","    \n","    # backward-prapagation\n","    loss.backward()\n","\n","    # update the parameters\n","    optimizer.step()\n","      \n","  if (epoch+1) % 20 == 0:\n","    print (f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.3f}')\n","\n","print(f'final loss: {loss.item():.3f}')"],"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["235 47\n","Epoch [20/200], Loss: 1.455\n","Epoch [40/200], Loss: 0.707\n","Epoch [60/200], Loss: 0.013\n","Epoch [80/200], Loss: 0.013\n","Epoch [100/200], Loss: 0.001\n","Epoch [120/200], Loss: 0.001\n","Epoch [140/200], Loss: 0.000\n","Epoch [160/200], Loss: 0.000\n","Epoch [180/200], Loss: 0.001\n","Epoch [200/200], Loss: 0.001\n","final loss: 0.001\n"]}]},{"cell_type":"code","metadata":{"id":"vzLaBzFk77QQ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1643236775076,"user_tz":0,"elapsed":174348,"user":{"displayName":"Ruei-Huan Rao","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13183932021173535053"}},"outputId":"9aa488f1-5ff3-4311-a6c8-a978f283e4ff"},"source":["model.eval()\n","with torch.no_grad():\n","  print(\"Let's chat! (type 'quit' to exit)\")\n","\n","  exit_list = ['exit', 'see you later', 'bye', 'quit', 'breat', 'q']\n","  while True:\n","    sentence = input(\"You: \")\n","    if sentence.lower() in exit_list:\n","      print(\"Bot: See you!\")\n","      break\n","\n","    sentence = tokenise(sentence)\n","    X = bag_of_words(sentence, all_words)\n","    X = X.reshape(1, X.shape[0])\n","    X = torch.from_numpy(X).to(device)\n","\n","    output = model(X)\n","    _, predicted = torch.max(output, dim=1)\n","\n","    tag = tags[predicted.item()]\n","\n","    probs = torch.softmax(output, dim=1)\n","    prob = probs[0][predicted.item()]\n","    if prob.item() > 0.6:\n","        for intent in intents['intents']:\n","            if tag == intent[\"tag\"]:\n","                print(f\"Bot: {random.choice(intent['responses'])}\")\n","    else:\n","        print(f\"Bot: My apologies, I don't get it...\")"],"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["Let's chat! (type 'quit' to exit)\n","You: hi\n","Bot: Hello, thanks for asking\n","You: where to go\n","Bot: Block-A 3rd floor 1st room\n","You: what should I do\n","Bot: I'm Bruno\n","You: what are you\n","Bot: My apologies, I don't get it...\n","You: q\n","Bot: See you!\n"]}]}]}