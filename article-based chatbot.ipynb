{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"article-based chatbot.ipynb","provenance":[{"file_id":"/v2/external/notebooks/snippets/importing_libraries.ipynb","timestamp":1621362738663}],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"0MTjf0P62e4_","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1643232811050,"user_tz":0,"elapsed":15577,"user":{"displayName":"Ruei-Huan Rao","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13183932021173535053"}},"outputId":"744c14b8-68ee-4667-cd3d-2d1c68d1185d"},"source":["pip install newspaper3k"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting newspaper3k\n","  Downloading newspaper3k-0.2.8-py3-none-any.whl (211 kB)\n","\u001b[?25l\r\u001b[K     |█▌                              | 10 kB 23.6 MB/s eta 0:00:01\r\u001b[K     |███                             | 20 kB 28.2 MB/s eta 0:00:01\r\u001b[K     |████▋                           | 30 kB 16.5 MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 40 kB 12.0 MB/s eta 0:00:01\r\u001b[K     |███████▊                        | 51 kB 11.4 MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 61 kB 11.8 MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 71 kB 8.9 MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 81 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 92 kB 10.9 MB/s eta 0:00:01\r\u001b[K     |███████████████▌                | 102 kB 9.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 112 kB 9.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 122 kB 9.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 133 kB 9.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 143 kB 9.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 153 kB 9.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 163 kB 9.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 174 kB 9.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 184 kB 9.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 194 kB 9.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 204 kB 9.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 211 kB 9.6 MB/s \n","\u001b[?25hRequirement already satisfied: lxml>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from newspaper3k) (4.2.6)\n","Requirement already satisfied: Pillow>=3.3.0 in /usr/local/lib/python3.7/dist-packages (from newspaper3k) (7.1.2)\n","Requirement already satisfied: PyYAML>=3.11 in /usr/local/lib/python3.7/dist-packages (from newspaper3k) (3.13)\n","Collecting tldextract>=2.0.1\n","  Downloading tldextract-3.1.2-py2.py3-none-any.whl (87 kB)\n","\u001b[K     |████████████████████████████████| 87 kB 7.5 MB/s \n","\u001b[?25hCollecting jieba3k>=0.35.1\n","  Downloading jieba3k-0.35.1.zip (7.4 MB)\n","\u001b[K     |████████████████████████████████| 7.4 MB 28.8 MB/s \n","\u001b[?25hCollecting feedparser>=5.2.1\n","  Downloading feedparser-6.0.8-py3-none-any.whl (81 kB)\n","\u001b[K     |████████████████████████████████| 81 kB 4.7 MB/s \n","\u001b[?25hRequirement already satisfied: requests>=2.10.0 in /usr/local/lib/python3.7/dist-packages (from newspaper3k) (2.23.0)\n","Collecting cssselect>=0.9.2\n","  Downloading cssselect-1.1.0-py2.py3-none-any.whl (16 kB)\n","Collecting feedfinder2>=0.0.4\n","  Downloading feedfinder2-0.0.4.tar.gz (3.3 kB)\n","Requirement already satisfied: beautifulsoup4>=4.4.1 in /usr/local/lib/python3.7/dist-packages (from newspaper3k) (4.6.3)\n","Collecting tinysegmenter==0.3\n","  Downloading tinysegmenter-0.3.tar.gz (16 kB)\n","Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.7/dist-packages (from newspaper3k) (2.8.2)\n","Requirement already satisfied: nltk>=3.2.1 in /usr/local/lib/python3.7/dist-packages (from newspaper3k) (3.2.5)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from feedfinder2>=0.0.4->newspaper3k) (1.15.0)\n","Collecting sgmllib3k\n","  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.10.0->newspaper3k) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.10.0->newspaper3k) (2021.10.8)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.10.0->newspaper3k) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.10.0->newspaper3k) (3.0.4)\n","Requirement already satisfied: filelock>=3.0.8 in /usr/local/lib/python3.7/dist-packages (from tldextract>=2.0.1->newspaper3k) (3.4.2)\n","Collecting requests-file>=1.4\n","  Downloading requests_file-1.5.1-py2.py3-none-any.whl (3.7 kB)\n","Building wheels for collected packages: tinysegmenter, feedfinder2, jieba3k, sgmllib3k\n","  Building wheel for tinysegmenter (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for tinysegmenter: filename=tinysegmenter-0.3-py3-none-any.whl size=13553 sha256=d431bd72d2214b9a0b5782cbbc838d74ad9bbc4474570a9571ee0d488e069c35\n","  Stored in directory: /root/.cache/pip/wheels/df/67/41/faca10fa501ca010be41b49d40360c2959e1c4f09bcbfa37fa\n","  Building wheel for feedfinder2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for feedfinder2: filename=feedfinder2-0.0.4-py3-none-any.whl size=3357 sha256=991be071f54960129d7b5220421e806d8dd8be8805710f1cbc92a37916f344ee\n","  Stored in directory: /root/.cache/pip/wheels/7f/d4/8f/6e2ca54744c9d7292d88ddb8d42876bcdab5e6d84a21c10346\n","  Building wheel for jieba3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for jieba3k: filename=jieba3k-0.35.1-py3-none-any.whl size=7398404 sha256=d67f768c85f3b61b61b97c0d927a8e058ec4d1b40034c5cd42b9c9d550ee63ae\n","  Stored in directory: /root/.cache/pip/wheels/4c/91/46/3c208287b726df325a5979574324878b679116e4baae1af3c3\n","  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6066 sha256=b45ff369fb33659a0bd6ddca42ec0fd6f5828da85eeb1f11b961a1af29359ab9\n","  Stored in directory: /root/.cache/pip/wheels/73/ad/a4/0dff4a6ef231fc0dfa12ffbac2a36cebfdddfe059f50e019aa\n","Successfully built tinysegmenter feedfinder2 jieba3k sgmllib3k\n","Installing collected packages: sgmllib3k, requests-file, tldextract, tinysegmenter, jieba3k, feedparser, feedfinder2, cssselect, newspaper3k\n","Successfully installed cssselect-1.1.0 feedfinder2-0.0.4 feedparser-6.0.8 jieba3k-0.35.1 newspaper3k-0.2.8 requests-file-1.5.1 sgmllib3k-1.0.0 tinysegmenter-0.3 tldextract-3.1.2\n"]}]},{"cell_type":"code","metadata":{"id":"B8N1qh1pxAyk","executionInfo":{"status":"ok","timestamp":1643232824696,"user_tz":0,"elapsed":11225,"user":{"displayName":"Ruei-Huan Rao","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13183932021173535053"}}},"source":["# import the libraries\n","import numpy as np\n","import random\n","\n","import torch\n","import nltk\n","nltk.download('punkt', quiet=True)\n","import tensorflow as tf\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.metrics.pairwise import cosine_similarity\n","\n","from newspaper import Article\n","\n","import warnings\n","warnings.filterwarnings('ignore')"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"nYGH9XE43iwF","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1643232836732,"user_tz":0,"elapsed":5756,"user":{"displayName":"Ruei-Huan Rao","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13183932021173535053"}},"outputId":"ee3ac71d-43d3-4a2e-ddea-e89909b4e5c7"},"source":["# download the article\n","article_input = input(\"Choose your fancy article link or press \\\"enter\\\" for the default article (about machine learning)\\n\")\n","default = ['', 'okay', 'default', 'ok', 'fine', '']\n","if article_input.lower() in default:\n","  url = 'https://en.wikipedia.org/wiki/Machine_learning'\n","else:\n","  url = str(article_input)\n","article = Article(url)\n","article.download()\n","article.parse()\n","article.nlp()\n","print(f'The topic we are talking about: \"{article.title}\"')\n","corpus = article.text"],"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Choose your fancy article link or press \"enter\" for the default article (about machine learning)\n","\n","The topic we are talking about: \"Machine learning\"\n"]}]},{"cell_type":"code","metadata":{"id":"tnZoSaKX8jOO","executionInfo":{"status":"ok","timestamp":1643232859121,"user_tz":0,"elapsed":242,"user":{"displayName":"Ruei-Huan Rao","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13183932021173535053"}}},"source":["# create a list of sentences\n","sentence_list = nltk.sent_tokenize(corpus)"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"oy3Qi_7bI0fT","executionInfo":{"status":"ok","timestamp":1643232862608,"user_tz":0,"elapsed":291,"user":{"displayName":"Ruei-Huan Rao","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13183932021173535053"}}},"source":["# a function to return a random greeting response to a user's greeting\n","def greeting_response(text):\n","  text = text.lower()\n","\n","  # bot's greeting reponse\n","  bot_greetings = [\"Hey :-)\", \"Hello, thanks for visiting\", \"Hi there, what can I do for you?\",\n","                   \"Hi there, how can I help?\", \"Hello, thanks for asking\", \"Good to see you again\"]\n","  # user's greeting\n","  user_greetings = ['hi', 'hey', 'hello']\n","\n","  for word in text.split():\n","    if word in user_greetings:\n","      return random.choice(bot_greetings)"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"id":"Y9bCAT2cK4AA","executionInfo":{"status":"ok","timestamp":1643233252033,"user_tz":0,"elapsed":257,"user":{"displayName":"Ruei-Huan Rao","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13183932021173535053"}}},"source":["def index_sort(list_var):\n","  index = list(range(0, len(list_var)))\n","\n","  for i in range(len(list_var)):\n","    for j in range(len(list_var)):\n","      if list_var[index[i]] > list_var[index[j]]:\n","        # swap\n","        index[i], index[j] = index[j], index[i]\n","  return index"],"execution_count":16,"outputs":[]},{"cell_type":"code","metadata":{"id":"zLrXBagGJs3P","executionInfo":{"status":"ok","timestamp":1643233254359,"user_tz":0,"elapsed":284,"user":{"displayName":"Ruei-Huan Rao","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13183932021173535053"}}},"source":["def bot_response(user_input):\n","  # lower the input and include it into the article\n","  user_input = user_input.lower()\n","  sentence_list.append(user_input)\n","\n","  # build bag of words of each of the sentences\n","  vectoriser = CountVectorizer()\n","  bag_of_words = vectoriser.fit_transform(sentence_list)\n","  \n","  # calculate the similarity (distance)\n","  similarity_scores = cosine_similarity(bag_of_words[-1], bag_of_words)\n","  similarity_scores_list = similarity_scores.flatten()\n","  \n","  # sort in accordance with the similarity score\n","  # remove the first sentence since it is the input itself\n","  index = index_sort(similarity_scores_list)\n","  index = index[1:]\n","  \n","  response = False\n","  \n","  # return all context which is similar with the input\n","  bot_res = ''\n","  for j, i in enumerate(range(len(index))):\n","    if similarity_scores_list[index[i]] > 0.0:\n","      bot_res = bot_res + ' ' + sentence_list[index[i]]\n","      response = True\n","      j += 1\n","    if j > 2:\n","      break\n","  \n","  if response == False:\n","    bot_res = bot_res + ' ' + \"My apologies, I don't get it...\"\n","\n","  # remove the input sentence\n","  sentence_list.remove(user_input)\n","\n","  return bot_res"],"execution_count":17,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PDwyfJX8MV6U","executionInfo":{"status":"ok","timestamp":1643233267725,"user_tz":0,"elapsed":9870,"user":{"displayName":"Ruei-Huan Rao","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13183932021173535053"}},"outputId":"6bc513e6-613e-499f-df4f-bb53e9f9fb61"},"source":["# execute the bot\n","exit_list = ['exit', 'see you later', 'bye', 'quit', 'breat', 'q']\n","print(\"bot: Hello! How can I help you?\")\n","while True:\n","  user_input = input()\n","  if user_input.lower() in exit_list:\n","    print('Bot: Chat with you later !')\n","    break\n","\n","  else:\n","    if greeting_response(user_input) != None:\n","      print('Bot: ' + greeting_response(user_input))\n","    else:\n","      print('Bot: ' + bot_response(user_input))"],"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["bot: Hello! How can I help you?\n","machine learning\n","Bot:  [123][124]\n","\n","Embedded Machine Learning [ edit ]\n","\n","Embedded Machine Learning is a sub-field of machine learning, where the machine learning model is run on embedded systems with limited computing resources such as wearable computers, edge devices and microcontrollers. Robot learning [ edit ]\n","\n","Robot learning is inspired by a multitude of machine learning methods, starting from supervised learning, reinforcement learning,[58][59] and finally meta-learning (e.g. Learning classifier systems (LCS) are a family of rule-based machine learning algorithms that combine a discovery component, typically a genetic algorithm, with a learning component, performing either supervised learning, reinforcement learning, or unsupervised learning.\n","q\n","Bot: Chat with you later !\n"]}]}]}