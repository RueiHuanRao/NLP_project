{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"transform chatbot.ipynb","private_outputs":true,"provenance":[],"collapsed_sections":[],"mount_file_id":"1_fHzy3y3opoVGuhcjtMxwgS3Xh9b9yee","authorship_tag":"ABX9TyMkau7Mu5H05IGDGCOQew3U"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"iLi8H-pfEw-A"},"source":["# $Building$ $a$ $Chatbot:$ $PyTorch$"]},{"cell_type":"code","metadata":{"id":"yOc_tW1TEgTK"},"source":["import csv\n","import random\n","import re\n","import unicodedata\n","import codecs\n","import numpy as np\n","import itertools\n","import math\n","import os\n","\n","import torch\n","import torch.nn as nn\n","from torch import optim\n","import torch.nn.functional as F\n","from torch.utils.data import Dataset, DataLoader\n","from torch.nn.utils.rnn import pad_sequence\n","from torch.utils.data.dataset import random_split\n","from torch.autograd import Variable\n","\n","from tqdm import tqdm_notebook, tqdm, notebook"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fUEz836QFSiL"},"source":["device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print(device)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0J7pD-uOFmvH"},"source":["# $Part$ $1:$ $Data$ $Preprocessing$"]},{"cell_type":"code","metadata":{"id":"wXnmWpUgFiY4"},"source":["path = '/content/drive/MyDrive/UCD_Programmes/Summer/ACM40960_Projects_in_Maths_Modelling/My_Project/dataset/cornell movie-dialogs corpus'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yIChZZR-A70-"},"source":["formatted_data_exist = False\n","\n","## reform the data structure\n","if formatted_data_exist == False:\n","  # visualize some lines\n","  with open(path + '/movie_lines.txt', 'r', encoding='iso-8859-1') as file:\n","    lines = file.readlines()\n","  for line in lines[:8]:\n","    print(line.strip())\n","  print('')\n","\n","  # split each line of the file into a dictionary of fields (lineID, characterID, movieID, character, text)\n","  line_fields = ['lineID', 'characterID', 'movieID', 'character', 'text']\n","  lines = {}\n","  with open(path + '/movie_lines.txt', 'r', encoding='iso-8859-1') as f:\n","    for line in f:\n","      values = line.split(' +++$+++ ')\n","      # extract fields\n","      lineObj = {}\n","      for i, field in enumerate(line_fields):\n","        lineObj[field] = values[i]\n","      lines[lineObj['lineID']] = lineObj\n","\n","\n","  # group fields of lines from 'loadlines' into conversation as based on 'movie_conversations.txt'\n","  conv_fields = ['character1ID', 'character2ID', 'movieID', 'utteranceIDs']\n","  conversations = []\n","  with open(path + '/movie_conversations.txt', 'r', encoding='iso-8859-1') as f:\n","    for line in f:\n","      values = line.split(' +++$+++')\n","\n","      # extract fields\n","      convObj = {}\n","      for i, field in enumerate(conv_fields):\n","        convObj[field] = values[i]\n","      \n","      # convert string result from split to list, since convObj['utteranceIDs'] == \"['L598485', 'L598486', ...]\"\n","      lineIDs = eval(convObj['utteranceIDs'])\n","\n","      # reassemble lines\n","      convObj['lines'] = []\n","      for lineID in lineIDs:\n","        convObj['lines'].append(lines[lineID])\n","      conversations.append(convObj)\n","\n","  # extracts pairs of sentences from conversations\n","  qa_pairs = []\n","  for conversation in conversations:\n","    # iterate over all the lines of the conversation\n","    for i in range(len(conversation['lines']) - 1):\n","      inputLine = conversation['lines'][i]['text'].strip()\n","      targetLine = conversation['lines'][i+1]['text'].strip()\n","      # filter wrong samples (if one of the lists is empty)\n","      if inputLine and targetLine:\n","        qa_pairs.append([inputLine, targetLine])\n","        \n","  print(qa_pairs[0]) # conversations"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EIJlAq8fQLiy"},"source":["# define path to new file\n","datafile = '/formatted_movie_lines.txt'\n","delimiter = '\\t'\n","# unescape the delimiter\n","delimiter = str(codecs.decode(delimiter, 'unicode_escape'))\n","\n","# create a new csv file\n","print('\\nWriting newly formatted file...')\n","with open(path + datafile, 'w', encoding='utf-8') as outputfile:\n","  writer = csv.writer(outputfile, delimiter=delimiter)\n","  for pair in qa_pairs:\n","    writer.writerow(pair)\n","\n","del conversation, conversations, qa_pairs\n","print(\"Done writing to file\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ti7kCoMrS6dt"},"source":["# visualize some formatted lines\n","with open(path + datafile, 'rb') as file:\n","  lines = file.readlines()\n","for line in lines[:8]:\n","  print(line)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"U1HvWp8lTdPP"},"source":["PAD_token = 0 # used for padding short sentences\n","SOS_token = 1 # start-of-sentence token\n","EOS_token = 2 # end-of-sentence token\n","\n","class Vocabulary:\n","  def __init__(self, name):\n","    self.name = name\n","    self.word2index = {}\n","    self.word2count = {}\n","    self.index2word = {PAD_token: \"PAD\", SOS_token: \"SOS\", EOS_token: \"EOS\"}\n","    self.num_words = 3 # PAD, SOS, EOS included\n","\n","  def addWord(self, word):\n","    '''\n","    include the words in the dictionary\n","    '''\n","    if word not in self.word2index:\n","      self.word2index[word] = self.num_words\n","      self.word2count[word] = 1\n","      self.index2word[self.num_words] = word\n","      self.num_words += 1\n","    else:\n","      self.word2count[word] += 1\n","  \n","  def addSentence(self, sentence):\n","    for word in sentence.split():\n","      self.addWord(word)\n","\n","  # remove words whose number are below a certain count threshold\n","  def trim(self, min_count):\n","    keep_words = []\n","    for k, v in self.word2count.items():\n","      if v >= min_count:\n","        keep_words.append(k)\n","    print(f'keep_words {len(keep_words)} / {len(self.word2index)} = {(len(keep_words) / len(self.word2index)):.4f}')\n","    # reinitualize dictionaries\n","    self.word2index = {}\n","    self.word2count = {}\n","    self.index2word = {PAD_token: 'PAD', SOS_token: 'SOS', EOS_token: 'EOS'}\n","    self.num_words = 3 # count default tokens\n","\n","    for word in keep_words:\n","      self.addWord(word)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4gM4_wu_YT6t"},"source":["# turn a unicode string to plain ASCII\n","def unicodeToASCII(s):\n","  return ''.join(c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn') # Mn stands for non-marking space"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4Xt2r7FcYpj6"},"source":["# lowercase, trim white space, lines...etc, and remove non-letter characters.\n","def normalizeString(s):\n","  s = unicodeToASCII(s.lower().strip())\n","  # replace any .!? by a whitespace + the character --> '!' = ' !'.\\1 means the first backend group --> [,!?]. r is to \n","  # not consider \\1 as a character (r to escape a backlash).\n","  s = re.sub(r\"([.!?])\", r\" \\1\", s)\n","  # remove any character that is not a sequence of lower or upper case letters + means one or more\n","  s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n","  # remove a sequence of whitespace charaters\n","  s = re.sub(r\"\\s+\", r\" \", s).strip()\n","  return s"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fxDwmX5SanIK"},"source":["print(\"reading and processing file... please wait\")\n","lines = open(path + datafile, encoding=\"utf-8\").read().strip().split('\\n')\n","print(lines[0])\n","# split every line into pairs and normalize\n","pairs = [[normalizeString(s) for s in pair.split('\\t')] for pair in lines]\n","print(\"Done reading!\")\n","\n","voc = Vocabulary(\"movie-dialogs corpus\") # the input is just a name here"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"33EXt1gKGvwB"},"source":["for pair in pairs[:3]:\n","  print(pair)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Xat-siNse02m"},"source":["# maximun sentence length to be considered (max words)\n","MAX_LENGTH = 15\n","def filterPair(p):\n","  '''\n","  return True if both sentences in a pair 'p' are under the MAX_LENGTH threshold\n","  '''\n","  return len(p[0].split()) < MAX_LENGTH and len(p[1].split()) < MAX_LENGTH\n","\n","def filterPairs(pairs):\n","  '''\n","  filter pairs using filterPair condition\n","  '''\n","  return [pair for pair in pairs if filterPair(pair)]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JrQ8uE1H3kaT"},"source":["pairs = [pair for pair in pairs if len(pair) > 1]\n","print(f\"There are {len(pairs)} pairs/conversations in the dataset\")\n","pairs = filterPairs(pairs)\n","print(f\"After filtering, there are {len(pairs)} pairs/conversations\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"b2wn7Zg24GPb"},"source":["# loop through each pair of and add the question and reply sentence to the vocabulary\n","for pair in pairs:\n","  voc.addSentence(pair[0])\n","  voc.addSentence(pair[1])\n","print(f\"There are {voc.num_words} words\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DiRoyl9u5KpE"},"source":["# minimum word count threshold for trimming \n","MIN_COUNT = 15\n","\n","def trimRareWords(voc, pairs, MIN_COUNT):\n","  '''\n","  trim words used under the MIN_COUNT from the voc\n","  '''\n","  voc.trim(MIN_COUNT)\n","  keep_pairs = []\n","  for pair in pairs:\n","    input_sentence = pair[0]\n","    output_sentence = pair[1]\n","    keep_input = True\n","    keep_output = True\n","\n","    # check input sentence\n","    for word in input_sentence.split():\n","      if word not in voc.word2index:\n","        keep_input = False\n","        break\n","    # check output sentence\n","    for word in output_sentence.split():\n","      if word not in voc.word2index:\n","        keep_output = False\n","        break\n","    \n","    # only keep pairs that do not contain trimmed word(s) in their input or output sentence\n","    if keep_input and keep_output:\n","      keep_pairs.append(pair)\n","\n","  print(f\"Trimmed from {len(pairs)} pairs to {len(keep_pairs)}, {len(keep_pairs)/len(pairs)} of total\")\n","  return keep_pairs\n","\n","# trim voc and pairs\n","pairs = trimRareWords(voc, pairs, MIN_COUNT)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"84_NR1Mll0lS"},"source":["print(pairs[0][0])\n","print(pairs[0][1])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hkTfoseOibMl"},"source":["# $Preparing$ $the$ $Data$ $for$ $Model$"]},{"cell_type":"code","metadata":{"id":"NZhu7_YqtS7O"},"source":["def indexesFromSentence(voc, sentence):\n","  \"\"\"\n","  inputs: (voc, sentence)\n","  output: words' indice, e.g. [1,3,5,3,2], \n","          where 2 symbolizes the end of the sentence, 1 indicates the start of the sentence\n","  \"\"\"\n","  return [SOS_token] + [voc.word2index[w] for w in sentence.split()] + [EOS_token]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"r9jP3bBItS7P"},"source":["class CorpusDataset(Dataset):\n","  \"\"\"\n","  customise a dataset\n","  input: (voc, sentence pairs)\n","  \"\"\"\n","  def __init__(self, voc, pairs):\n","    self.x = [indexesFromSentence(voc, q[0]) for q in pairs]\n","    self.y = [indexesFromSentence(voc, a[1]) for a in pairs]\n","    self.n_samples = len(self.x)\n","\n","  def __getitem__(self, index):\n","    return self.x[index], self.y[index]\n","\n","  def __len__(self):\n","    return self.n_samples"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xuxJJg8jtS7P"},"source":["# create the dataset \n","dataset = CorpusDataset(voc, pairs)\n","\n","# delete the variables to release the memory\n","del pairs, pair, lines"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-X13Ufq8tS7Q"},"source":["def collate(batch, src_pad, trg_pad, device):\n","  '''\n","  fill padding tokens to let the sentences are in the same size\n","  '''\n","  inputs = [torch.LongTensor(item[0]) for item in batch]\n","  targets = [torch.LongTensor(item[1]) for item in batch]\n","  \n","  # Pad sequencse so that they are all the same length (within one minibatch)\n","  padded_inputs = pad_sequence(inputs, padding_value=src_pad, batch_first=True)\n","  padded_targets = pad_sequence(targets, padding_value=trg_pad, batch_first=True)\n","  \n","  # Sort by length for CUDA optimizations\n","  lengths = torch.LongTensor([len(x) for x in inputs])\n","  lengths, permutation = lengths.sort(dim=0, descending=True)\n","\n","  return padded_inputs[permutation].to(device), padded_targets[permutation].to(device), lengths.to(device)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nQZZsfQKtS7Q"},"source":["batch_size = 1024\n","n_iterations = math.ceil(len(dataset)/batch_size)\n","data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, \n","                        collate_fn = lambda batch: collate(batch, PAD_token, PAD_token, device)) # fit the size within the small batch\n","\n","n_iterations"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OsqT-jqW2pNO"},"source":["# $Building$ $the$ $Model$"]},{"cell_type":"code","metadata":{"id":"rpewmXSfIOw7"},"source":["class SelfAttention(nn.Module):\n","    def __init__(self, embed_size, heads):\n","        super().__init__()\n","        self.embed_size = embed_size\n","        self.heads = heads\n","        self.head_dim = embed_size // heads # dq, dk, and dv\n","\n","        assert (self.head_dim * heads == embed_size), \"Embedding size needs to be divisible by heads\"\n","\n","        self.values = nn.Linear(self.head_dim, self.head_dim, bias=False)\n","        self.keys = nn.Linear(self.head_dim, self.head_dim, bias=False)\n","        self.queries = nn.Linear(self.head_dim, self.head_dim, bias=False)\n","        self.fc_out = nn.Linear(embed_size, embed_size) # fully connected NN\n","\n","    def forward(self, values, keys, query, mask):\n","        # Get number of training examples\n","        N = query.shape[0]\n","\n","        value_len, key_len, query_len = values.shape[1], keys.shape[1], query.shape[1]\n","\n","        # Split the embedding into self.heads different pieces\n","        values = values.reshape(N, value_len, self.heads, self.head_dim)\n","        keys = keys.reshape(N, key_len, self.heads, self.head_dim)\n","        query = query.reshape(N, query_len, self.heads, self.head_dim)\n","\n","        values = self.values(values)  # (N, value_len, heads, head_dim)\n","        keys = self.keys(keys)  # (N, key_len, heads, head_dim)\n","        queries = self.queries(query)  # (N, query_len, heads, heads_dim)\n","\n","        # Einsum does matrix mult. for query*keys for each training example\n","        # with every other training example, don't be confused by einsum\n","        # it's just how I like doing matrix multiplication & bmm\n","\n","        energy = torch.einsum(\"nqhd,nkhd->nhqk\", [queries, keys])\n","        # queries shape: (N, query_len, heads, heads_dim),\n","        # keys shape: (N, key_len, heads, heads_dim)\n","        # energy: (N, heads, query_len, key_len)\n","\n","        # Mask padded indices so their weights become 0\n","        if mask is not None:\n","            energy = energy.masked_fill(mask == 0, float(\"-1e20\"))\n","\n","        # Normalize energy values similarly to seq2seq + attention\n","        # so that they sum to 1. Also divide by scaling factor for\n","        # better stability\n","        attention = torch.softmax(energy / (self.embed_size ** (1 / 2)), dim=3)\n","        # attention shape: (N, heads, query_len, key_len)\n","\n","        out = torch.einsum(\"nhql,nlhd->nqhd\", [attention, values]).reshape(\n","            N, query_len, self.heads * self.head_dim\n","        )\n","        # attention shape: (N, heads, query_len, key_len)\n","        # values shape: (N, value_len, heads, heads_dim)\n","        # out after matrix multiply: (N, query_len, heads, head_dim), then\n","        # we reshape and flatten the last two dimensions.\n","\n","        out = self.fc_out(out)\n","        # Linear layer doesn't modify the shape, final shape will be\n","        # (N, query_len, embed_size)\n","\n","        return out\n","\n","\n","class EncoderBlock(nn.Module):\n","    def __init__(self, embed_size, heads, dropout, forward_expansion):\n","        super().__init__()\n","        self.attention = SelfAttention(embed_size, heads) # output: (N, query_len, embed_size)\n","        self.norm = nn.LayerNorm(embed_size)\n","        # self.norm2 = nn.LayerNorm(embed_size)\n","\n","        self.feed_forward = nn.Sequential(\n","            nn.Linear(embed_size, forward_expansion * embed_size),\n","            nn.ReLU(),\n","            nn.Linear(forward_expansion * embed_size, embed_size),\n","        )\n","\n","        self.dropout = nn.Dropout(dropout)\n","\n","    def forward(self, value, key, query, mask):\n","        attention = self.attention(value, key, query, mask)\n","\n","        # Add skip connection, run through normalization and finally dropout\n","        x = self.dropout(self.norm(attention + query)) \n","        forward = self.feed_forward(x)\n","        out = self.dropout(self.norm(forward + x)) # output: (N, query_len, embed_size)\n","        return out\n","\n","\n","class Encoder(nn.Module):\n","    def __init__(\n","        self,\n","        src_vocab_size,\n","        embed_size,\n","        num_layers,\n","        heads,\n","        device,\n","        forward_expansion,\n","        dropout,\n","        max_length,\n","    ):\n","        super().__init__()\n","        self.embed_size = embed_size\n","        self.device = device\n","        self.word_embedding = nn.Embedding(src_vocab_size, embed_size) # output: (src_voc_size, embed_size)\n","        self.position_embedding = nn.Embedding(max_length, embed_size) # output: (max_length, embed_size)\n","\n","        self.layers = nn.ModuleList(\n","            [\n","                EncoderBlock(\n","                    embed_size,\n","                    heads,\n","                    dropout=dropout,\n","                    forward_expansion=forward_expansion,\n","                )\n","                for _ in range(num_layers)\n","            ]\n","        )\n","\n","        self.dropout = nn.Dropout(dropout)\n","\n","    def forward(self, x, mask):\n","        N, seq_length = x.shape\n","        positions = torch.arange(0, seq_length).expand(N, seq_length).to(self.device)\n","        out = self.dropout(\n","            (self.word_embedding(x) + self.position_embedding(positions))\n","        )\n","\n","        # In the Encoder the query, key, value are all the same\n","        for layer in self.layers:\n","            out = layer(out, out, out, mask)\n","\n","        return out\n","\n","\n","class DecoderBlock(nn.Module):\n","    def __init__(self, embed_size, heads, forward_expansion, dropout, device):\n","        super().__init__()\n","        self.norm = nn.LayerNorm(embed_size)\n","        self.attention = SelfAttention(embed_size, heads=heads)\n","        self.encoder_block = EncoderBlock(embed_size, heads, dropout, forward_expansion)\n","        self.dropout = nn.Dropout(dropout)\n","\n","    def forward(self, x, value, key, src_mask, trg_mask):\n","        attention = self.attention(x, x, x, trg_mask)\n","        query = self.dropout(self.norm(attention + x))\n","        out = self.encoder_block(value, key, query, src_mask)\n","        return out\n","\n","\n","class Decoder(nn.Module):\n","    def __init__(\n","        self,\n","        trg_vocab_size,\n","        embed_size,\n","        num_layers,\n","        heads,\n","        forward_expansion,\n","        dropout,\n","        device,\n","        max_length,\n","    ):\n","        super().__init__()\n","        self.device = device\n","        self.word_embedding = nn.Embedding(trg_vocab_size, embed_size)\n","        self.position_embedding = nn.Embedding(max_length, embed_size)\n","\n","        self.layers = nn.ModuleList(\n","            [\n","                DecoderBlock(embed_size, heads, forward_expansion, dropout, device)\n","                for _ in range(num_layers)\n","            ]\n","        )\n","        self.fc_out = nn.Linear(embed_size, trg_vocab_size)\n","        self.dropout = nn.Dropout(dropout)\n","\n","    def forward(self, x, enc_out, src_mask, trg_mask):\n","        N, seq_length = x.shape\n","        positions = torch.arange(0, seq_length).expand(N, seq_length).to(self.device)\n","        x = self.dropout((self.word_embedding(x) + self.position_embedding(positions)))\n","\n","        for layer in self.layers:\n","            x = layer(x, enc_out, enc_out, src_mask, trg_mask)\n","\n","        out = self.fc_out(x)\n","        prob_out = F.softmax(out, dim=-1)\n","\n","        return out, prob_out\n","\n","\n","class Transformer(nn.Module):\n","    def __init__(\n","        self,\n","        src_vocab_size,\n","        trg_vocab_size,\n","        src_pad_idx,\n","        trg_pad_idx,\n","        embed_size=512,\n","        num_layers=8,\n","        forward_expansion=4,\n","        heads=8,\n","        dropout=0.2,\n","        device=\"cpu\",\n","        max_length=100,\n","    ):\n","\n","        super().__init__()\n","\n","        self.encoder = Encoder(\n","            src_vocab_size,\n","            embed_size,\n","            num_layers,\n","            heads,\n","            device,\n","            forward_expansion,\n","            dropout,\n","            max_length,\n","        )\n","\n","        self.decoder = Decoder(\n","            trg_vocab_size,\n","            embed_size,\n","            num_layers,\n","            heads,\n","            forward_expansion,\n","            dropout,\n","            device,\n","            max_length,\n","        )\n","\n","        self.src_pad_idx = src_pad_idx\n","        self.trg_pad_idx = trg_pad_idx\n","        self.device = device\n","\n","    def make_src_mask(self, src):\n","        src_mask = (src != self.src_pad_idx).unsqueeze(1).unsqueeze(2)\n","        # (N, 1, 1, src_len)\n","        return src_mask.to(self.device)\n","\n","    def make_trg_mask(self, trg):\n","        N, trg_len = trg.shape\n","        trg_mask = torch.tril(torch.ones((trg_len, trg_len))).expand(\n","            N, 1, trg_len, trg_len\n","        )\n","        return trg_mask.to(self.device)\n","\n","    def forward(self, src, trg):\n","        src_mask = self.make_src_mask(src)\n","        trg_mask = self.make_trg_mask(trg)\n","        enc_src = self.encoder(src, src_mask)\n","        out, prob_out = self.decoder(trg, enc_src, src_mask, trg_mask)\n","        return out, prob_out"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Hl4rPBDFtnfZ"},"source":["save_model = False\n","load_model = False\n","\n","print(device)\n","model = Transformer(voc.num_words, voc.num_words, PAD_token, PAD_token, device=device).to(device)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"32s9I63cIWgd"},"source":["# define the loss function\n","loss_function = nn.CrossEntropyLoss(ignore_index = PAD_token)\n","\n","# set hyper-parameters\n","learning_rate = 0.01\n","epoches = 1\n","\n","# set updating function\n","optimiser = torch.optim.SGD(model.parameters(), lr = learning_rate)\n","\n","# train the model\n","for epoch in range(epoches):\n","  total_loss = total = 0\n","\n","  progress_bar = notebook.tqdm(data_loader, desc=\"Training\", leave=False)\n","  for i, (inputs, targets, lengths) in enumerate(progress_bar):\n","    # initialise gradients\n","    optimiser.zero_grad()\n","\n","    # forwards\n","    output, _ = model(inputs.to(device), targets[:, :-1].to(device))\n","    pred = output.view(-1, output.size(-1)) # pred = (sentence_length, ebeded_dim)\n","    y = targets[:, 1:].contiguous().view(-1) # y = (sentence_length)\n","\n","    # loss\n","    loss = loss_function(pred, y)\n","\n","    # backpropagation\n","    loss.backward()\n","    \n","    # update the weights\n","    optimiser.step()\n","\n","    # record loss\n","    if (i+1) % 3 == 0:\n","      print(f'epoch {epoch+1:2d}/{epoches}, step {i+1:3d}/{n_iterations}, loss {loss:.3f}')\n","  tqdm.write(f'epoch\\t #{epoch + 1:2d}\\t  train_loss: {loss:.3f}\\n')\n","\n","if save_model:\n","  ## save the model paramaters\n","  # define the parameter dictionary\n","  checkpoint = {\n","      'epoch': 1,\n","      'model_state': model.state_dict(),\n","      'optim_state': optimiser.state_dict()\n","  }\n","\n","  # save the parameters\n","  torch.save(checkpoint, './checkpoint.pth')\n","\n","if load_model:\n","  ## load parameters\n","  load_path = '/content/drive/MyDrive/UCD_Programmes/Summer/ACM40960_Projects_in_Maths_Modelling/My_Project/dataset/'\n","  loaded_checkpoint = torch.load(os.path.join(load_path, \"checkpoint.pth\"), map_location=device)\n","  epoch = loaded_checkpoint[\"epoch\"]\n","  model.load_state_dict(loaded_checkpoint[\"model_state\"])\n","  optimiser.load_state_dict(loaded_checkpoint[\"optim_state\"])\n","  model.to(device)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0PxqH6zm2yIR"},"source":["def sentenceFromIndice(voc, indice):\n","  return \" \".join([voc.index2word[idx] for idx in indice])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"niq-PVKwHdS8"},"source":["## execute the model\n","model.eval() # turn off the dropout layers\n","with torch.no_grad(): # stop updating the weights\n","  print(\"Let's chat! (type 'quit' to exit)\")\n","\n","  exit_list = ['exit', 'see you later', 'bye', 'quit', 'breat', 'q']\n","  while True:\n","    sentence = input(\"You: \")\n","    if sentence.lower() in exit_list:\n","      print(\"Bot: See you!\")\n","      break\n","\n","    # modify the input sentence\n","    sentence = normalizeString(sentence)\n","    voc.addSentence(sentence)\n","    inputs = torch.tensor([indexesFromSentence(voc, sentence)]).to(device)\n","\n","    # forward\n","    output, _ = model(inputs.to(device), inputs.to(device))\n","    print(f\"Bot: {sentenceFromIndice(voc, [output[0,i].data.topk(1)[1].item() for i in range(output.shape[1])])}\")"],"execution_count":null,"outputs":[]}]}